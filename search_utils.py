# search_utils.py
import requests
from bs4 import BeautifulSoup
from langdetect import detect
from urllib.parse import quote

def translate_query(query, lang_out="th"):
    """‡πÅ‡∏õ‡∏• query ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡πÉ‡∏ä‡πâ Google Translate API ‡∏ü‡∏£‡∏µ)"""
    try:
        resp = requests.get(
            f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl={lang_out}&dt=t&q={quote(query)}"
        )
        if resp.status_code == 200:
            return resp.json()[0][0][0]
        return query
    except Exception:
        return query

def fetch_google_web(query, lang_out="th", max_results=3):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏à‡∏≤‡∏Å Google"""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        )
    }
    url = f"https://www.google.com/search?q={quote(query)}&hl={lang_out}"
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "lxml")
        results = []
        for g in soup.select("div.g"):
            title = g.select_one("h3")
            link = g.select_one("a")
            if title and link:
                url_result = link['href']
                if url_result.startswith("/url?q="):
                    url_result = url_result.split("/url?q=")[1].split("&")[0]
                url_result = url_result.split("&sa=")[0]
                results.append(f"‚Ä¢ {title.text.strip()}\n{url_result}")
            if len(results) >= max_results:
                break
        # Featured Answer/Knowledge Panel
        if not results:
            featured = soup.select_one("div[data-attrid='wa:/description'] span")
            if featured:
                return [f"Google Featured Answer:\n{featured.text.strip()}"]
            return ["‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏à‡∏≤‡∏Å Google ‡∏Ñ‡∏£‡∏±‡∏ö"]
        return results
    except Exception as e:
        return [f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ß‡πá‡∏ö: {str(e)}"]

def fetch_google_news(query, lang_out="th", max_results=3):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß Google News"""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        )
    }
    url = f"https://www.google.com/search?q={quote(query)}&hl={lang_out}&tbm=nws"
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "lxml")
        news_results = []
        for n in soup.select("div.g"):
            title = n.select_one("h3")
            link = n.select_one("a")
            snippet = n.select_one("div.BNeawe.s3v9rd.AP7Wnd")
            if title and link:
                url_news = link['href']
                if url_news.startswith("/url?q="):
                    url_news = url_news.split("/url?q=")[1].split("&")[0]
                summary = snippet.text.strip() if snippet else ""
                news_results.append(f"‚Ä¢ {title.text.strip()}\n{url_news}\n{summary}")
            if len(news_results) >= max_results:
                break
        if not news_results:
            return ["‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Google News ‡∏Ñ‡∏£‡∏±‡∏ö"]
        return news_results
    except Exception as e:
        return [f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß: {str(e)}"]

def fetch_google_images(query, lang_out="th", max_results=3):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å Google Images (‡∏™‡πà‡∏á url ‡∏£‡∏π‡∏õ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó‡πÉ‡∏ä‡πâ/preview)"""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        )
    }
    url = f"https://www.google.com/search?q={quote(query)}&hl={lang_out}&tbm=isch"
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "lxml")
        image_results = []
        for img in soup.select("img"):
            src = img.get("src")
            # ‡∏î‡∏∂‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏£‡∏π‡∏õ‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏£‡∏π‡∏õ‡πÇ‡∏•‡πÇ‡∏Å‡πâ/‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô/empty
            if src and src.startswith("http"):
                image_results.append(src)
            if len(image_results) >= max_results:
                break
        if not image_results:
            return ["‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å Google Images ‡∏Ñ‡∏£‡∏±‡∏ö"]
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö markdown ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó preview ‡πÑ‡∏î‡πâ (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ plain url ‡∏Å‡πá‡πÑ‡∏î‡πâ)
        return [f"![image]({url})" if url else url for url in image_results]
    except Exception as e:
        return [f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {str(e)}"]

def smart_search(query, lang_out="th", max_results=3, enable_news=True, enable_images=True):
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Google (‡πÄ‡∏ß‡πá‡∏ö+‡∏Ç‡πà‡∏≤‡∏ß+‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û)
    :param query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    :param lang_out: ‡∏†‡∏≤‡∏©‡∏≤ (default 'th')
    :param max_results: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î/‡∏´‡∏°‡∏ß‡∏î
    :param enable_news: ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
    :param enable_images: ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ
    :return: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° list[str] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ GPT-4o/‡∏ö‡∏≠‡∏ó
    """
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ (‡∏´‡∏£‡∏∑‡∏≠ lang_out)
        lang = detect(query)
        if lang != lang_out:
            query = translate_query(query, lang_out=lang_out)
        all_results = []
        # ‡πÄ‡∏ß‡πá‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        web_results = fetch_google_web(query, lang_out=lang_out, max_results=max_results)
        if web_results:
            all_results.append("üîé **‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Google Search:**\n" + "\n\n".join(web_results))
        # ‡∏Ç‡πà‡∏≤‡∏ß
        if enable_news:
            news_results = fetch_google_news(query, lang_out=lang_out, max_results=max_results)
            if news_results:
                all_results.append("üì∞ **‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:**\n" + "\n\n".join(news_results))
        # ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        if enable_images:
            img_results = fetch_google_images(query, lang_out=lang_out, max_results=max_results)
            if img_results:
                all_results.append("üñºÔ∏è **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û:**\n" + "\n".join(img_results))
        if not all_results:
            return ["‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google ‡∏Ñ‡∏£‡∏±‡∏ö"]
        return all_results
    except Exception as e:
        return [f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}"]

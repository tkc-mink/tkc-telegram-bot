# utils/news_utils.py
# -*- coding: utf-8 -*-
"""
Utility ‡∏î‡∏∂‡∏á "‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î" ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡∏°‡∏µ API key ‡∏Å‡πá‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°:
1) internal_tools.google_search (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‚Äî ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà
2) Google News RSS (public, ‡πÑ‡∏°‡πà‡∏°‡∏µ key) ‚Äî ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
3) Mock (‡∏≠‡∏≠‡∏ü‡πÑ‡∏•‡∏ô‡πå/‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢) ‚Äî ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏•‡πà‡∏°

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥:
- HTTP retry + backoff
- Timeout ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô ENV
- ‡πÅ‡∏Ñ‡∏ä‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (TTL)
- ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏†‡∏≤‡∏©‡∏≤/‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏î‡πâ (‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏ó‡∏¢)
- ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Markdown ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå (‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ï‡πà‡∏≠ Markdown)
"""

from __future__ import annotations
from typing import List, Dict, Any, Optional, Tuple
import os
import time
import re
import html
import requests
from urllib.parse import quote
from xml.etree import ElementTree as ET

# =======================
# Tunables / ENV
# =======================
TIMEOUT = float(os.getenv("NEWS_TIMEOUT_SEC", "10"))
RETRIES = int(os.getenv("NEWS_RETRIES", "2"))
BACKOFF_BASE = float(os.getenv("NEWS_BACKOFF_BASE_SEC", "0.4"))
CACHE_TTL = float(os.getenv("NEWS_CACHE_TTL_SEC", "300"))

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/125.0.0.0 Safari/537.36"
)

# =======================
# Internal search (optional)
# =======================
# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°: internal_tools.google_search.search(queries=[...], search_type='NEWS')
_internal_search = None
try:
    from internal_tools import google_search  # type: ignore
    _internal_search = google_search
except Exception:
    _internal_search = None  # ‡∏à‡∏∞ fallback ‡πÄ‡∏õ‡πá‡∏ô RSS

# =======================
# In-memory cache (TTL)
# =======================
_cache: Dict[str, Tuple[float, Any]] = {}

def _cache_get(key: str):
    v = _cache.get(key)
    if not v:
        return None
    ts, val = v
    if time.time() - ts <= CACHE_TTL:
        return val
    _cache.pop(key, None)
    return None

def _cache_put(key: str, val: Any):
    _cache[key] = (time.time(), val)

# =======================
# HTTP helpers
# =======================
_session = requests.Session()
_session.headers.update({"User-Agent": UA})

def _retry_sleep(attempt: int):
    delay = BACKOFF_BASE * (2 ** max(0, attempt - 1)) + 0.05 * attempt
    time.sleep(min(delay, 2.5))

def _http_get(url: str, params: Optional[Dict[str, Any]] = None) -> Optional[requests.Response]:
    last_err = None
    for attempt in range(1, RETRIES + 2):
        try:
            r = _session.get(url, params=params or {}, timeout=TIMEOUT)
            if r.ok:
                return r
            last_err = f"HTTP {r.status_code}"
            if attempt <= RETRIES:
                _retry_sleep(attempt)
        except requests.RequestException as e:
            last_err = str(e)
            if attempt <= RETRIES:
                _retry_sleep(attempt)
    print(f"[news_utils] _http_get give up: {url} ({last_err})")
    return None

# =======================
# Markdown helpers
# =======================
_MD_CODE = re.compile(r"([`*_~\[\](){}#+\-.!|>])")

def _md_escape(s: str) -> str:
    """‡∏´‡∏ô‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞ Markdown ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ü‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ï"""
    s = s or ""
    return _MD_CODE.sub(r"\\\1", s)

def _clean_text(s: str, max_len: int = 180) -> str:
    s = html.unescape(s or "")
    # ‡∏•‡∏ö‡πÅ‡∏ó‡πá‡∏Å HTML ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    if len(s) > max_len:
        s = s[: max_len - 1] + "‚Ä¶"
    return s

# =======================
# Google News RSS
# =======================
def _google_news_rss_url(topic: Optional[str], lang: str, region: str) -> str:
    """
    topic=None ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î' -> ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®/‡∏†‡∏≤‡∏©‡∏≤
    ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: ‡πÉ‡∏ä‡πâ /rss/search?q=...
    """
    ceid = f"{region}:{lang}"
    if not topic or topic.strip() in {"‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", "latest", "headlines"}:
        return f"https://news.google.com/rss?hl={lang}&gl={region}&ceid={ceid}"
    q = quote(topic)
    return f"https://news.google.com/rss/search?q={q}&hl={lang}&gl={region}&ceid={ceid}"

def _parse_google_news_rss(xml_text: str, limit: int = 3) -> List[Dict[str, str]]:
    # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á RSS: <item><title>, <link>, <pubDate>, <source>, <description>
    items: List[Dict[str, str]] = []
    try:
        root = ET.fromstring(xml_text)
        for it in root.findall(".//item"):
            title = (it.findtext("title") or "").strip()
            link = (it.findtext("link") or "").strip()
            source_el = it.find("{http://www.w3.org/2005/Atom}source") or it.find("source")
            source = (source_el.text or "").strip() if source_el is not None else ""
            desc = (it.findtext("description") or "").strip()
            if title and link:
                items.append({
                    "title": title,
                    "link": link,
                    "snippet": _clean_text(desc),
                    "source": source or "Google News",
                })
            if len(items) >= limit:
                break
    except Exception as e:
        print(f"[news_utils] RSS parse error: {e}")
    return items

def _fetch_google_news(topic: Optional[str], lang: str, region: str, limit: int) -> List[Dict[str, str]]:
    url = _google_news_rss_url(topic, lang, region)
    cache_key = f"gn:{lang}:{region}:{topic or 'headlines'}:{limit}"
    cached = _cache_get(cache_key)
    if cached is not None:
        return cached
    resp = _http_get(url)
    if not resp:
        return []
    articles = _parse_google_news_rss(resp.text, limit=limit)
    _cache_put(cache_key, articles)
    return articles

# =======================
# Internal search wrapper
# =======================
def _fetch_internal(topic: str, limit: int) -> List[Dict[str, str]]:
    if not _internal_search:
        return []
    try:
        results = _internal_search.search(queries=[topic], search_type="NEWS")  # type: ignore
        # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: results[0].results -> list ‡∏Ç‡∏≠‡∏á object ‡∏°‡∏µ title/link/snippet/source
        if not results or not getattr(results[0], "results", None):
            return []
        out: List[Dict[str, str]] = []
        for item in results[0].results[:limit]:
            out.append({
                "title": str(getattr(item, "title", "")),
                "link": str(getattr(item, "link", "")),
                "snippet": str(getattr(item, "snippet", "")),
                "source": str(getattr(item, "source", "")),
            })
        return out
    except Exception as e:
        print(f"[news_utils] internal search failed: {e}")
        return []

# =======================
# Public API
# =======================
def get_news(
    topic: str = "‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î",
    *,
    lang: str = "th",
    region: str = "TH",
    max_items: int = 3,
) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ñ‡πâ‡∏≤ topic='‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î')
    ‡∏Ñ‡∏∑‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á Markdown ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå

    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
        get_news()                  -> ‡∏û‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏ó‡∏¢‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        get_news("‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢")     -> ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢
        get_news("AI", lang="en", region="US", max_items=5)
    """
    topic = (topic or "").strip()
    print(f"[news_utils] Fetching news: topic='{topic}', lang={lang}, region={region}, max={max_items}")

    # 1) internal search ‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    articles = _fetch_internal(topic, max_items)
    # 2) fallback ‚Üí Google News RSS
    if not articles:
        articles = _fetch_google_news(topic, lang=lang, region=region, limit=max_items)

    # 3) mock offline ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏á
    if not articles:
        articles = [
            {
                "title": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß: ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≠‡∏ü‡πÑ‡∏•‡∏ô‡πå",
                "link": "https://example.com/1",
                "snippet": "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡πÑ‡∏î‡πâ",
                "source": "Offline",
            },
            {
                "title": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß 2",
                "link": "https://example.com/2",
                "snippet": "‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó‡∏¢‡∏±‡∏á‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏•‡πà‡∏°",
                "source": "Offline",
            },
        ][:max_items]

    # ‡∏à‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Markdown
    header = f"üóûÔ∏è **‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡πà‡∏ô**" if topic in {"‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", "", None} else f"üóûÔ∏è **‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡πà‡∏ô‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {_md_escape(topic)}**"
    parts: List[str] = [header]

    for a in articles[:max_items]:
        title = _md_escape(a.get("title", "").strip() or "-")
        link = a.get("link", "").strip() or "#"
        source = _md_escape(a.get("source", "").strip() or "")
        snippet = _md_escape(_clean_text(a.get("snippet", "") or "", max_len=180))

        parts.append(
            f"‚Ä¢ **{title}**\n"
            + (f"  _{source}_\n" if source else "")
            + (f"  {snippet}\n" if snippet else "")
            + f"  üîó [{_md_escape('‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠')}]({link})"
        )

    return "\n\n".join(parts)
